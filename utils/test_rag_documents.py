"""
RAG æ–‡æ¡£åŠ è½½å’Œåˆ‡åˆ†æµ‹è¯•
å…ˆä¸è¿›è¡Œå‘é‡åŒ–ï¼Œä»…æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½
"""

from pathlib import Path
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader


def main():
    print("=" * 60)
    print("ğŸ“š RAG æ–‡æ¡£å¤„ç†æµ‹è¯•")
    print("=" * 60)
    print()

    docs_directory = "docs/ragfiles"
    documents = []
    file_stats = []

    print("1. æ–‡ä»¶æ‰«æ")
    print("-" * 60)

    dir_path = Path(docs_directory)
    if not dir_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {docs_directory}")
        return

    # éå†æ–‡ä»¶
    for file_path in dir_path.iterdir():
        if file_path.is_file():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"ğŸ“„ {file_path.name} ({size_mb:.2f} MB)")
            file_stats.append({
                'name': file_path.name,
                'size_mb': size_mb,
                'path': str(file_path)
            })

    print()
    print("2. æ–‡æ¡£åŠ è½½")
    print("-" * 60)

    for file_stat in file_stats:
        file_path = Path(file_stat['path'])
        print(f"\nğŸ“„ æ­£åœ¨åŠ è½½: {file_stat['name']}")

        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
            if file_path.suffix.lower() == '.pdf':
                loader = PyPDFLoader(str(file_path))
                file_docs = loader.load()
                print(f"   âœ… PDF åŠ è½½æˆåŠŸ: {len(file_docs)} é¡µ")
            elif file_path.suffix.lower() in ['.docx', '.doc']:
                loader = Docx2txtLoader(str(file_path))
                file_docs = loader.load()
                print(f"   âœ… Word åŠ è½½æˆåŠŸ: {len(file_docs)} é¡µ")
            elif file_path.suffix.lower() == '.xlsx':
                print(f"   âš ï¸  Excel æ–‡ä»¶æš‚ä¸æ”¯æŒ: è·³è¿‡")
                continue
            else:
                print(f"   âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: è·³è¿‡")
                continue

            documents.extend(file_docs)

        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            continue

    print()
    print(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å¯¹è±¡")
    print()

    print("3. æ–‡æ¡£åˆ‡åˆ†")
    print("-" * 60)

    if not documents:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£")
        return

    # ç»Ÿè®¡æ–‡æ¡£å†…å®¹
    total_chars = 0
    for doc in documents:
        total_chars += len(doc.page_content)

    print(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   - æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print()

    # åˆ‡åˆ†æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )

    splits = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(splits)} ä¸ªæ–‡æœ¬å—")
    print()

    # æ˜¾ç¤ºå‰ 3 ä¸ªæ–‡æœ¬å—ç¤ºä¾‹
    print("4. æ–‡æœ¬å—ç¤ºä¾‹")
    print("-" * 60)

    for i, split in enumerate(splits[:3], 1):
        print(f"\næ–‡æœ¬å— {i}:")
        print(f"æ¥æº: {split.metadata.get('source', 'æœªçŸ¥')}")
        print(f"é•¿åº¦: {len(split.page_content)} å­—ç¬¦")
        print(f"å†…å®¹: {split.page_content[:150]}...")

    print()
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print()

    print("âœ… æ–‡æ¡£å¤„ç†æµ‹è¯•é€šè¿‡")
    print(f"   - æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"   - åˆ‡åˆ†ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
    print()
    print("ğŸ¯ å¯ä»¥ç»§ç»­è¿›è¡Œå‘é‡åº“æ„å»º")
    print()
    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. ä½¿ç”¨ ChromaDB æ„å»ºå‘é‡åº“")
    print("   2. æµ‹è¯•è¯­ä¹‰æœç´¢åŠŸèƒ½")
    print("   3. é›†æˆåˆ° RAG æ™ºèƒ½ä½“")


if __name__ == "__main__":
    main()
