"""
æ··åˆæ£€ç´¢å¢å¼ºæ¨¡å—
Day 2: è¯­ä¹‰æ£€ç´¢ + å…³é”®è¯æ£€ç´¢æ··åˆç­–ç•¥

å®ç°ä¸¤ç§æ£€ç´¢æ–¹å¼çš„èåˆï¼š
1. è¯­ä¹‰æ£€ç´¢ (Similarity Search): åŸºäºå‘é‡ç›¸ä¼¼åº¦
2. å…³é”®è¯æ£€ç´¢ (BM25/å…³é”®è¯åŒ¹é…): åŸºäºè¯é¢‘åŒ¹é…
3. æ··åˆèåˆ (RRF): Reciprocal Rank Fusion ç®—æ³•

å‚è€ƒæ•™å­¦è®¡åˆ’ Day 2 è¦æ±‚ï¼š
- æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå…³é”®è¯æ£€ç´¢+è¯­ä¹‰æ£€ç´¢ï¼‰
- è¯­ä¹‰åˆ‡åˆ†åŸåˆ™ï¼ˆæŒ‰è¯­ä¹‰å®Œæ•´æ€§ã€å›ºå®šé•¿åº¦+é‡å çª—å£ï¼‰
- Chroma DB çš„æ ¸å¿ƒç‰¹æ€§
"""

import os
import re
from typing import List, Dict, Tuple
from collections import Counter
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter


class HybridRetriever:
    """
    æ··åˆæ£€ç´¢å™¨
    
    ç»“åˆè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ï¼Œä½¿ç”¨ RRF ç®—æ³•èåˆç»“æœ
    """
    
    def __init__(
        self,
        vector_store: Chroma,
        embedding_model: str = "BAAI/bge-m3",
        semantic_weight: float = 0.6,
        keyword_weight: float = 0.4,
        k: int = 5
    ):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        
        Args:
            vector_store: Chroma å‘é‡æ•°æ®åº“å®ä¾‹
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            semantic_weight: è¯­ä¹‰æ£€ç´¢æƒé‡ (0-1)
            keyword_weight: å…³é”®è¯æ£€ç´¢æƒé‡ (0-1)
            k: è¿”å›ç»“æœæ•°é‡
        """
        self.vector_store = vector_store
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
        self.k = k
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
            show_progress=False
        )
    
    def keyword_search(
        self,
        query: str,
        documents: List[Document],
        top_k: int = 10
    ) -> List[Tuple[Document, float]]:
        """
        å…³é”®è¯æ£€ç´¢ - åŸºäº BM25 ç®€åŒ–ç‰ˆæœ¬
        
        ç®—æ³•ï¼š
        1. æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯
        2. è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„è¯é¢‘å¾—åˆ†
        3. è¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å¾…æ£€ç´¢çš„æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            [(æ–‡æ¡£, å¾—åˆ†), ...]
        """
        # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        query_terms = self._extract_terms(query)
        
        if not query_terms:
            return []
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„å¾—åˆ†
        scores = []
        for doc in documents:
            doc_text = doc.page_content.lower()
            doc_terms = self._extract_terms(doc_text)
            
            # è®¡ç®— TF (è¯é¢‘)
            score = 0
            for term in query_terms:
                # ç²¾ç¡®åŒ¹é…å¾—åˆ†æ›´é«˜
                exact_count = doc_text.count(term)
                # éƒ¨åˆ†åŒ¹é…
                partial_count = sum(1 for t in doc_terms if term in t or t in term)
                
                score += exact_count * 2 + partial_count * 0.5
            
            scores.append((doc, score))
        
        # æŒ‰å¾—åˆ†æ’åº
        scores.sort(key=lambda x: x[1], reverse=True)
        
        return scores[:top_k]
    
    def _extract_terms(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # åœç”¨è¯åˆ—è¡¨
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
            'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were'
        }
        
        # æå–ä¸­æ–‡å’Œè‹±æ–‡è¯æ±‡
        # ä¸­æ–‡ï¼š2-4 ä¸ªå­—ç¬¦çš„è¯
        chinese_terms = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
        # è‹±æ–‡ï¼šé•¿åº¦ >= 3 çš„è¯
        english_terms = re.findall(r'[a-zA-Z]{3,}', text.lower())
        
        all_terms = chinese_terms + english_terms
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_terms = [
            term for term in all_terms 
            if term not in stopwords and len(term) >= 2
        ]
        
        return filtered_terms
    
    def reciprocal_rank_fusion(
        self,
        semantic_results: List[Tuple[Document, float]],
        keyword_results: List[Tuple[Document, float]],
        k: int = 60
    ) -> List[Tuple[Document, float]]:
        """
        RRF (Reciprocal Rank Fusion) èåˆç®—æ³•
        
        å…¬å¼ï¼šscore = Î£ 1 / (k + rank)
        
        å…¶ä¸­ï¼š
        - k: å¸¸æ•°ï¼Œé€šå¸¸å– 60
        - rank: æ–‡æ¡£åœ¨æŸä¸ªåˆ—è¡¨ä¸­çš„æ’åï¼ˆä» 1 å¼€å§‹ï¼‰
        
        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ [(doc, score), ...]
            keyword_results: å…³é”®è¯æ£€ç´¢ç»“æœ [(doc, score), ...]
            k: RRF å¸¸æ•°
            
        Returns:
            èåˆåçš„ç»“æœ [(doc, rrf_score), ...]
        """
        # åˆ›å»ºæ–‡æ¡£åˆ°æ’åçš„æ˜ å°„
        semantic_ranks = {
            id(doc): rank + 1 
            for rank, (doc, _) in enumerate(semantic_results)
        }
        keyword_ranks = {
            id(doc): rank + 1 
            for rank, (doc, _) in enumerate(keyword_results)
        }
        
        # è·å–æ‰€æœ‰å”¯ä¸€æ–‡æ¡£
        all_docs = set()
        for doc, _ in semantic_results:
            all_docs.add(id(doc))
        for doc, _ in keyword_results:
            all_docs.add(id(doc))
        
        # è®¡ç®— RRF å¾—åˆ†
        rrf_scores = []
        
        # ä»è¯­ä¹‰ç»“æœä¸­è·å–æ–‡æ¡£å¯¹è±¡
        doc_map = {id(doc): doc for doc, _ in semantic_results + keyword_results}
        
        for doc_id in all_docs:
            score = 0.0
            
            # è¯­ä¹‰æ£€ç´¢å¾—åˆ†
            if doc_id in semantic_ranks:
                rank = semantic_ranks[doc_id]
                score += self.semantic_weight * (1.0 / (k + rank))
            
            # å…³é”®è¯æ£€ç´¢å¾—åˆ†
            if doc_id in keyword_ranks:
                rank = keyword_ranks[doc_id]
                score += self.keyword_weight * (1.0 / (k + rank))
            
            rrf_scores.append((doc_map[doc_id], score))
        
        # æŒ‰ RRF å¾—åˆ†æ’åº
        rrf_scores.sort(key=lambda x: x[1], reverse=True)
        
        return rrf_scores
    
    def hybrid_search(
        self,
        query: str,
        documents: List[Document],
        k: int = None
    ) -> List[Tuple[Document, float]]:
        """
        æ··åˆæ£€ç´¢ - èåˆè¯­ä¹‰æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢
        
        æµç¨‹ï¼š
        1. è¯­ä¹‰æ£€ç´¢è·å–ç›¸å…³æ–‡æ¡£
        2. å…³é”®è¯æ£€ç´¢è·å–ç›¸å…³æ–‡æ¡£
        3. ä½¿ç”¨ RRF ç®—æ³•èåˆç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            [(æ–‡æ¡£, èåˆå¾—åˆ†), ...]
        """
        k = k or self.k
        
        print(f"ğŸ” æ‰§è¡Œæ··åˆæ£€ç´¢: '{query}'")
        print(f"   å€™é€‰æ–‡æ¡£æ•°: {len(documents)}")
        print(f"   è¯­ä¹‰æƒé‡: {self.semantic_weight}, å…³é”®è¯æƒé‡: {self.keyword_weight}")
        
        # 1. è¯­ä¹‰æ£€ç´¢
        print("\nğŸ“Š æ­¥éª¤ 1: è¯­ä¹‰æ£€ç´¢...")
        semantic_results = self._semantic_search(query, documents, top_k=10)
        print(f"   âœ… è·å– {len(semantic_results)} ä¸ªè¯­ä¹‰æ£€ç´¢ç»“æœ")
        
        # 2. å…³é”®è¯æ£€ç´¢
        print("\nğŸ”¤ æ­¥éª¤ 2: å…³é”®è¯æ£€ç´¢...")
        keyword_results = self.keyword_search(query, documents, top_k=10)
        print(f"   âœ… è·å– {len(keyword_results)} ä¸ªå…³é”®è¯æ£€ç´¢ç»“æœ")
        
        # 3. RRF èåˆ
        print("\nğŸ”„ æ­¥éª¤ 3: RRF èåˆ...")
        fused_results = self.reciprocal_rank_fusion(semantic_results, keyword_results)
        print(f"   âœ… èåˆåå…± {len(fused_results)} ä¸ªç»“æœ")
        
        # è¿”å› Top K
        return fused_results[:k]
    
    def _semantic_search(
        self,
        query: str,
        documents: List[Document],
        top_k: int = 10
    ) -> List[Tuple[Document, float]]:
        """
        è¯­ä¹‰æ£€ç´¢ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£
            top_k: è¿”å›æ•°é‡
            
        Returns:
            [(æ–‡æ¡£, ç›¸ä¼¼åº¦å¾—åˆ†), ...]
        """
        # å°†æ–‡æ¡£ä¸´æ—¶æ·»åŠ åˆ°å‘é‡åº“
        temp_store = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            collection_name="temp_hybrid"
        )
        
        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
        results = temp_store.similarity_search_with_score(query, k=top_k)
        
        # è½¬æ¢æ ¼å¼
        return [(doc, score) for doc, score in results]


def test_hybrid_retrieval():
    """
    æµ‹è¯•æ··åˆæ£€ç´¢åŠŸèƒ½
    """
    print("=" * 60)
    print("ğŸ§ª æ··åˆæ£€ç´¢æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        Document(page_content="""
        å›¾ä¹¦é¦†å€Ÿé˜…è§„åˆ™
        
        æœ¬ç§‘ç”Ÿæœ€å¤šå¯å€Ÿ 10 æœ¬å›¾ä¹¦ï¼Œå€Ÿé˜…æœŸé™ä¸º 30 å¤©ï¼Œå¯ç»­å€Ÿä¸€æ¬¡ï¼ˆ15 å¤©ï¼‰ã€‚
        é€¾æœŸå›¾ä¹¦æ¯æœ¬æ¯å¤©ç½šæ¬¾ 0.5 å…ƒã€‚é—å¤±å›¾ä¹¦éœ€ç…§ä»·èµ”å¿ã€‚
        å­¦æ ¡æä¾› CNKIã€ä¸‡æ–¹ç­‰å­¦æœ¯æ•°æ®åº“ï¼Œå¯åœ¨æ ¡å›­ç½‘å†…å…è´¹è®¿é—®ã€‚
        """, metadata={"source": "library_rules"}),
        
        Document(page_content="""
        æ–°ç”ŸæŠ¥åˆ°æŒ‡å—
        
        æ–°ç”ŸæŠ¥åˆ°æ—¶é—´ï¼šæ¯å¹´ 9 æœˆ 1 æ—¥è‡³ 9 æœˆ 5 æ—¥
        æŠ¥åˆ°åœ°ç‚¹ï¼šå­¦æ ¡ä¸»æ¥¼å¤§å…
        æ‰€éœ€ææ–™ï¼šå½•å–é€šçŸ¥ä¹¦ã€èº«ä»½è¯åŸä»¶åŠå¤å°ä»¶ã€é«˜è€ƒå‡†è€ƒè¯ã€è¿‘æœŸä¸€å¯¸å…å† ç…§ç‰‡ 8 å¼ 
        æŠ¥åˆ°å½“æ—¥å¯åŠç†æ ¡å›­ä¸€å¡é€šå’Œå®¿èˆå…¥ä½æ‰‹ç»­
        """, metadata={"source": "enrollment_guide"}),
        
        Document(page_content="""
        å®¿èˆç®¡ç†è§„å®š
        
        å®¿èˆå¼€æ”¾æ—¶é—´ï¼šæ¯å¤© 6:00 - 23:00
        é—¨ç¦æ—¶é—´ï¼šæ™šä¸Š 23:00ï¼Œå‘¨æœ«å»¶é•¿è‡³ 24:00
        å®¿èˆåˆ†é…ï¼šæŒ‰ç…§é™¢ç³»å’Œç­çº§ç»Ÿä¸€åˆ†é…
        å®¿èˆè®¾æ–½ï¼šæ¯ä¸ªå®¿èˆé…å¤‡ç©ºè°ƒã€ç‹¬ç«‹å«ç”Ÿé—´ã€ä¹¦æ¡Œå’Œè¡£æŸœ
        """, metadata={"source": "dormitory_rules"}),
        
        Document(page_content="""
        å¥–å­¦é‡‘è¯„å®šæ ‡å‡†
        
        å›½å®¶å¥–å­¦é‡‘ï¼šæ¯äººæ¯å¹´ 8000 å…ƒ
        è¯„å®šæ¡ä»¶ï¼šç»¼åˆç´ è´¨æµ‹è¯„æˆç»©æ’ååœ¨å‰ 5%ï¼Œæ— æŒ‚ç§‘è®°å½•ï¼Œç§¯æå‚ä¸ç¤¾ä¼šå®è·µæ´»åŠ¨
        ç”³è¯·æ—¶é—´ï¼šæ¯å­¦å¹´ç§‹å­£å­¦æœŸï¼ˆ10 æœˆ-11 æœˆï¼‰
        """, metadata={"source": "scholarship"}),
        
        Document(page_content="""
        é€‰è¯¾ä¸å­¦åˆ†åˆ¶åº¦
        
        å­¦åˆ†è¦æ±‚ï¼šæœ¬ç§‘ç”Ÿéœ€ä¿®æ»¡ 160 å­¦åˆ†æ–¹å¯æ¯•ä¸š
        è¯¾ç¨‹ç±»å‹ï¼šå…¬å…±åŸºç¡€è¯¾ï¼ˆçº¦ 40 å­¦åˆ†ï¼‰ã€ä¸“ä¸šåŸºç¡€è¯¾ï¼ˆçº¦ 60 å­¦åˆ†ï¼‰
        é€‰è¯¾æ—¶é—´ï¼šæ¯å­¦æœŸç¬¬ 1 å‘¨ä¸ºé€‰è¯¾å‘¨ï¼Œç¬¬ 2 å‘¨ä¸ºè¡¥é€‰å’Œé€€é€‰æ—¶é—´
        """, metadata={"source": "course_selection"})
    ]
    
    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¸€ä¸ªç©ºçš„å‘é‡åº“ä½œä¸ºå ä½ç¬¦
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True},
        show_progress=False
    )
    
    temp_store = Chroma.from_documents(
        documents=[Document(page_content="placeholder")],
        embedding=embeddings
    )
    
    retriever = HybridRetriever(
        vector_store=temp_store,
        semantic_weight=0.6,
        keyword_weight=0.4,
        k=3
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å›¾ä¹¦é¦†å€Ÿä¹¦æœ‰ä»€ä¹ˆè§„å®šï¼Ÿ",
        "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å¸¦ä»€ä¹ˆï¼Ÿ",
        "å®¿èˆæ™šä¸Šå‡ ç‚¹å…³é—¨ï¼Ÿ",
        "æ€ä¹ˆç”³è¯·å¥–å­¦é‡‘ï¼Ÿ"
    ]
    
    for query in test_queries:
        print("=" * 60)
        print(f"ğŸ“ æŸ¥è¯¢: {query}")
        print("=" * 60)
        
        results = retriever.hybrid_search(query, test_docs, k=3)
        
        print("\nğŸ“‹ æ£€ç´¢ç»“æœ:")
        for i, (doc, score) in enumerate(results, 1):
            print(f"\nã€ç»“æœ {i}ã€‘èåˆå¾—åˆ†: {score:.4f}")
            print(f"æ¥æº: {doc.metadata.get('source', 'unknown')}")
            print(f"å†…å®¹: {doc.page_content[:150]}...")
        
        print()
    
    print("=" * 60)
    print("âœ… æ··åˆæ£€ç´¢æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    test_hybrid_retrieval()
