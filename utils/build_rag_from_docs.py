"""
ä½¿ç”¨çœŸå®æ–‡æ¡£æ„å»º RAG å‘é‡åº“
ä» docs/ragfiles ç›®å½•åŠ è½½æ–‡æ¡£å¹¶æ„å»º Chroma å‘é‡æ•°æ®åº“
"""

import os
from pathlib import Path
from typing import List

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
import chromadb


def load_documents_from_directory(directory: str) -> List:
    """
    ä»ç›®å½•åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£

    Args:
        directory: æ–‡æ¡£ç›®å½•è·¯å¾„

    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    documents = []
    dir_path = Path(directory)

    if not dir_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return documents

    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md'}

    # éå†ç›®å½•
    for file_path in dir_path.iterdir():
        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½: {file_path.name}")

            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                if file_path.suffix.lower() == '.pdf':
                    loader = PyPDFLoader(str(file_path))
                    file_docs = loader.load()
                elif file_path.suffix.lower() in ['.docx', '.doc']:
                    loader = Docx2txtLoader(str(file_path))
                    file_docs = loader.load()
                elif file_path.suffix.lower() in ['.txt', '.md']:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                        file_docs = [Document(page_content=text, metadata={'source': str(file_path)})]

                documents.extend(file_docs)
                print(f"   âœ… åŠ è½½æˆåŠŸ: {len(file_docs)} é¡µ")

            except Exception as e:
                print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
                continue

    return documents


def build_vector_store_from_documents(
    documents: List,
    persist_directory: str = "./db/chroma_db_campus",
    collection_name: str = "campus_documents"
):
    """
    ä»æ–‡æ¡£åˆ—è¡¨æ„å»ºå‘é‡åº“

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        persist_directory: æŒä¹…åŒ–ç›®å½•
        collection_name: é›†åˆåç§°
    """
    print()
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ„å»º RAG å‘é‡åº“")
    print("=" * 60)
    print()

    # 1. åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )

    # 2. åˆ‡åˆ†æ–‡æ¡£
    print("ğŸ“ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
    splits = text_splitter.split_documents(documents)
    print(f"âœ… æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(splits)} ä¸ªæ–‡æœ¬å—")
    print()

    # 3. åˆ›å»º Chroma å®¢æˆ·ç«¯
    print("ğŸ’¾ æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
    client = chromadb.PersistentClient(path=persist_directory)

    # 4. åˆ›å»ºé›†åˆ
    collection = client.get_or_create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"}
    )

    print(f"âœ… Chroma é›†åˆåˆ›å»º/åŠ è½½æˆåŠŸ")
    print(f"   æŒä¹…åŒ–ç›®å½•: {persist_directory}")
    print(f"   é›†åˆåç§°: {collection_name}")
    print()

    # 5. å‡†å¤‡æ•°æ®
    print("ğŸ“Š æ­£åœ¨å‡†å¤‡å‘é‡æ•°æ®...")
    ids = [f"doc_{i}" for i in range(len(splits))]
    texts = [split.page_content for split in splits]
    metadatas = [split.metadata for split in splits]

    # æ¸…ç©ºé›†åˆï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
    try:
        collection.delete(ids=collection.get()['ids'])
        print("ğŸ§¹ æ¸…ç©ºæ—§æ•°æ®")
    except:
        pass

    # 6. æ‰¹é‡æ·»åŠ æ–‡æ¡£
    print(f"æ­£åœ¨æ·»åŠ  {len(texts)} ä¸ªæ–‡æœ¬å—...")
    batch_size = 100  # æ¯æ‰¹å¤„ç† 100 ä¸ª

    for i in range(0, len(texts), batch_size):
        batch_ids = ids[i:i+batch_size]
        batch_texts = texts[i:i+batch_size]
        batch_metadatas = metadatas[i:i+batch_size]

        collection.add(
            ids=batch_ids,
            documents=batch_texts,
            metadatas=batch_metadatas
        )

        print(f"   è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)} ({min((i+batch_size)/len(texts)*100, 100):.1f}%)")

    print(f"âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
    print()

    print("=" * 60)
    print("ğŸ‰ RAG å‘é‡åº“æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print()
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   - æ–‡æœ¬å—æ•°é‡: {len(splits)}")
    print(f"   - é›†åˆåç§°: {collection_name}")
    print(f"   - æŒä¹…åŒ–ç›®å½•: {persist_directory}")
    print()

    return client, collection


def test_semantic_search(collection, queries: List[str]):
    """
    æµ‹è¯•è¯­ä¹‰æœç´¢

    Args:
        collection: Chroma é›†åˆ
        queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    """
    print("=" * 60)
    print("ğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢")
    print("=" * 60)
    print()

    for i, query in enumerate(queries, 1):
        print(f"æŸ¥è¯¢ {i}: {query}")
        print("-" * 60)

        try:
            results = collection.query(
                query_texts=[query],
                n_results=3
            )

            if results['documents'] and results['documents'][0]:
                for j, (doc, metadata) in enumerate(zip(
                    results['documents'][0],
                    results['metadatas'][0]
                ), 1):
                    print(f"\nç»“æœ {j}:")
                    print(f"æ¥æº: {metadata.get('source', 'æœªçŸ¥')}")
                    print(f"å†…å®¹: {doc[:150]}...")
            else:
                print("\næœªæ‰¾åˆ°ç›¸å…³ç»“æœ")

        except Exception as e:
            print(f"\næŸ¥è¯¢å‡ºé”™: {e}")

        print()

    print("=" * 60)
    print("âœ… è¯­ä¹‰æœç´¢æµ‹è¯•å®Œæˆ")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    docs_directory = "docs/ragfiles"
    persist_directory = "./db/chroma_db_campus"
    collection_name = "campus_documents"

    print("=" * 60)
    print("ğŸ“š ä½¿ç”¨çœŸå®æ–‡æ¡£æ„å»º RAG å‘é‡åº“")
    print("=" * 60)
    print()

    # 1. åŠ è½½æ–‡æ¡£
    documents = load_documents_from_directory(docs_directory)

    if not documents:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç¨‹åºé€€å‡º")
        return

    # 2. æ„å»ºå‘é‡åº“
    client, collection = build_vector_store_from_documents(
        documents=documents,
        persist_directory=persist_directory,
        collection_name=collection_name
    )

    # 3. æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ",
        "å­¦ç”Ÿè¿çºªæœ‰å“ªäº›å¤„ç½šè§„å®šï¼Ÿ",
        "ç¡•å£«ç ”ç©¶ç”Ÿæ‹›ç”Ÿçš„åŸºæœ¬æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å­¦æ ¡æœ‰å“ªäº›é‡ç‚¹å®éªŒå®¤ï¼Ÿ",
        "å®¿èˆç®¡ç†å’Œç”Ÿæ´»è§„å®š"
    ]

    print()
    test_semantic_search(collection, test_queries)

    # 4. ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    print()
    print("ğŸ’¡ æç¤º:")
    print("   - å‘é‡åº“å·²æŒä¹…åŒ–åˆ°ç£ç›˜")
    print("   - ä¸‹æ¬¡å¯ä»¥ç›´æ¥åŠ è½½ä½¿ç”¨")
    print("   - ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½å‘é‡åº“:")
    print(f"""
       import chromadb

       client = chromadb.PersistentClient(path="{persist_directory}")
       collection = client.get_collection(name="{collection_name}")
       """)


if __name__ == "__main__":
    main()
