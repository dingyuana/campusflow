# å‘é‡åº“åŸºæœ¬æ¦‚å¿µè¯¦è§£

## ğŸ“‹ æ¦‚è¿°

å‘é‡åº“ï¼ˆVector Databaseï¼‰æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨ã€æ£€ç´¢å’Œè®¡ç®—å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿã€‚åœ¨ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿä¸­ï¼Œå‘é‡åº“æ˜¯å®ç°è¯­ä¹‰æœç´¢çš„æ ¸å¿ƒç»„ä»¶ã€‚

## ğŸ” æ ¸å¿ƒæ¦‚å¿µ

### 1. Embeddingï¼ˆåµŒå…¥/å‘é‡åŒ–ï¼‰

#### ä»€ä¹ˆæ˜¯ Embeddingï¼Ÿ

**Embedding** æ˜¯å°†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„æŠ€æœ¯ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
- **é«˜ç»´ç©ºé—´**ï¼šé€šå¸¸å°†æ•°æ®æ˜ å°„åˆ° 128-3072 ç»´çš„ç©ºé—´
- **è¯­ä¹‰ä¿ç•™**ï¼šç›¸ä¼¼çš„æ•°æ®åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
- **æ•°å€¼è¡¨ç¤º**ï¼šæ¯ä¸ªç»´åº¦ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼Œæ•´ä½“å‘é‡å’Œè¡¨ç¤ºæ•°æ®çš„è¯­ä¹‰

#### Embedding çš„åŸç†

```
æ–‡æœ¬ â†’ Embedding æ¨¡å‹ â†’ å‘é‡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"çŒ«"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [0.23, -0.15, 0.87, ..., 0.12]
"ç‹—"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [0.21, -0.18, 0.91, ..., 0.09]
"æ±½è½¦" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [0.89, 0.73, -0.12, ..., -0.45]
```

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼š
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰ï¼š-1 åˆ° 1ï¼Œ1 è¡¨ç¤ºå®Œå…¨ç›¸åŒ
- æ¬§æ°è·ç¦»ï¼ˆEuclidean Distanceï¼‰ï¼šè·ç¦»è¶Šå°è¶Šç›¸ä¼¼
- æ›¼å“ˆé¡¿è·ç¦»ï¼ˆManhattan Distanceï¼‰ï¼šåŸå¸‚è¡—åŒºè·ç¦»

#### å¸¸ç”¨ Embedding æ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `BAAI/bge-small` | 512 | å¿«é€Ÿã€è½»é‡ | å¿«é€ŸåŸå‹ |
| `BAAI/bge-base` | 768 | å¹³è¡¡æ€§èƒ½å’Œè´¨é‡ | é€šç”¨åœºæ™¯ |
| `BAAI/bge-m3` | 1024 | å¤šè¯­è¨€ã€é«˜è´¨é‡ | ä¸­è‹±åŒè¯­ |
| `text-embedding-3-large` | 3072 | è´¨é‡æœ€ä½³ | å•†ä¸šåº”ç”¨ |
| `all-MiniLM-L6-v2` | 384 | è¶…è½»é‡ | è¾¹ç¼˜è®¾å¤‡ |

#### Python å®ç°

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹
model = SentenceTransformer('BAAI/bge-m3')

# ç”Ÿæˆæ–‡æœ¬å‘é‡
sentences = [
    "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ",
    "å­¦æ ¡æœ‰å“ªäº›é‡ç‚¹å®éªŒå®¤ï¼Ÿ"
]

embeddings = model.encode(sentences)

print(f"å‘é‡ç»´åº¦: {embeddings.shape[1]}")
print(f"ç¬¬ä¸€ä¸ªå‘é‡: {embeddings[0][:5]}...")
```

**è¾“å‡º**ï¼š
```
å‘é‡ç»´åº¦: 1024
ç¬¬ä¸€ä¸ªå‘é‡: [0.0234 -0.1567 0.8721 ...]
```

---

### 2. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆSimilarity Matchingï¼‰

#### ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰

**å…¬å¼**ï¼š
```
cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
```

**ç‰¹ç‚¹**ï¼š
- ä¸å—å‘é‡é•¿åº¦å½±å“
- èŒƒå›´ï¼š-1 åˆ° 1
- 1 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0 è¡¨ç¤ºæ— å…³ï¼Œ-1 è¡¨ç¤ºç›¸å

#### Python å®ç°

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    return dot_product / (norm1 * norm2)

# ç¤ºä¾‹
vec1 = np.array([0.23, -0.15, 0.87])
vec2 = np.array([0.21, -0.18, 0.91])

similarity = cosine_similarity(vec1, vec2)
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")
```

**è¾“å‡º**ï¼š
```
ç›¸ä¼¼åº¦: 0.9978
```

#### æ¬§æ°è·ç¦»ï¼ˆEuclidean Distanceï¼‰

**å…¬å¼**ï¼š
```
d(A, B) = âˆš(Î£(Ai - Bi)Â²)
```

**Python å®ç°**ï¼š
```python
def euclidean_distance(vec1, vec2):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„æ¬§æ°è·ç¦»
    """
    return np.linalg.norm(vec1 - vec2)
```

#### ç›¸ä¼¼åº¦ vs è·ç¦»

| æŒ‡æ ‡ | è¯´æ˜ | èŒƒå›´ | è¶Šå¤§è¡¨ç¤º |
|------|------|------|----------|
| ä½™å¼¦ç›¸ä¼¼åº¦ | å‘é‡å¤¹è§’çš„ä½™å¼¦å€¼ | -1 åˆ° 1 | è¶Šç›¸ä¼¼ |
| æ¬§æ°è·ç¦» | å‘é‡åœ¨ç©ºé—´ä¸­çš„ç›´çº¿è·ç¦» | 0 åˆ° âˆ | è¶Šä¸ç›¸ä¼¼ |

---

## ğŸ—„ï¸ å‘é‡åº“æ¶æ„

### æ ¸å¿ƒåŠŸèƒ½

1. **å‘é‡æ’å…¥**ï¼ˆInsertï¼‰
   - å°†å‘é‡å’Œå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨
   - è‡ªåŠ¨åˆ›å»ºç´¢å¼•

2. **å‘é‡æ£€ç´¢**ï¼ˆSearchï¼‰
   - ç›¸ä¼¼åº¦æœç´¢ï¼ˆKNNï¼‰
   - èŒƒå›´æœç´¢
   - æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰

3. **å‘é‡æ›´æ–°**ï¼ˆUpdateï¼‰
   - æ›´æ–°å‘é‡å€¼
   - æ›´æ–°å…ƒæ•°æ®

4. **å‘é‡åˆ é™¤**ï¼ˆDeleteï¼‰
   - æŒ‰å‘é‡ ID åˆ é™¤
   - æ‰¹é‡åˆ é™¤

### ç´¢å¼•ç»“æ„

#### HNSWï¼ˆHierarchical Navigable Small Worldï¼‰

**ç‰¹ç‚¹**ï¼š
- åˆ†å±‚å›¾ç»“æ„
- è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNï¼‰
- é«˜æ€§èƒ½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®

**å·¥ä½œåŸç†**ï¼š
```
Layer 2: â”€â”€â”€â”€â”€Aâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€C
Layer 1: â”€â”€â”€â”€Aâ”€â”€â”€Dâ”€â”€â”€Bâ”€â”€â”€Eâ”€â”€â”€Câ”€â”€â”€Fâ”€â”€â”€
Layer 0: A-D-E-B-G-C-F-H-I-J-K...
```

#### IVFï¼ˆInverted File Indexï¼‰

**ç‰¹ç‚¹**ï¼š
- èšç±» + å€’æ’ç´¢å¼•
- å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
- é€‚åˆä¸­ç­‰è§„æ¨¡æ•°æ®

---

## ğŸ’¾ ChromaDB å®æˆ˜

### å®‰è£…å’Œåˆå§‹åŒ–

```bash
# å®‰è£… ChromaDB
uv pip install chromadb
```

```python
import chromadb

# åˆ›å»ºå®¢æˆ·ç«¯
client = chromadb.Client()

# åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="campus_knowledge",
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)
```

### æ·»åŠ å‘é‡

```python
# æ·»åŠ æ–‡æ¡£å‘é‡
collection.add(
    ids=["doc1", "doc2", "doc3"],
    embeddings=[
        [0.23, -0.15, 0.87, ...],  # å‘é‡ 1
        [0.21, -0.18, 0.91, ...],  # å‘é‡ 2
        [0.89, 0.73, -0.12, ...],  # å‘é‡ 3
    ],
    documents=[
        "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡å½•å–é€šçŸ¥ä¹¦ã€èº«ä»½è¯ç­‰ææ–™",
        "å­¦æ ¡æœ‰å¤šä¸ªé‡ç‚¹å®éªŒå®¤ï¼ŒåŒ…æ‹¬äººå·¥æ™ºèƒ½å®éªŒå®¤ç­‰",
        "å­¦ç”Ÿè¿çºªåˆ†ä¸ºè­¦å‘Šã€ä¸¥é‡è­¦å‘Šã€è®°è¿‡ç­‰"
    ],
    metadatas=[
        {"source": "æŠ¥åˆ°æ‰‹å†Œ.pdf", "page": 5},
        {"source": "é™¢æ ¡ç®€ä»‹.docx", "page": 1},
        {"source": "è¿çºªè§„å®š.pdf", "page": 3}
    ]
)
```

### ç›¸ä¼¼åº¦æœç´¢

```python
# æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
results = collection.query(
    query_texts=["æŠ¥åˆ°éœ€è¦ä»€ä¹ˆææ–™ï¼Ÿ"],
    n_results=3
)

# è¾“å‡ºç»“æœ
for i, (doc, metadata) in enumerate(zip(
    results['documents'][0],
    results['metadatas'][0]
), 1):
    print(f"ç»“æœ {i}:")
    print(f"æ–‡æ¡£: {doc}")
    print(f"æ¥æº: {metadata['source']}, é¡µç : {metadata['page']}")
    print(f"è·ç¦»: {results['distances'][0][i-1]:.4f}")
    print()
```

**è¾“å‡º**ï¼š
```
ç»“æœ 1:
æ–‡æ¡£: æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡å½•å–é€šçŸ¥ä¹¦ã€èº«ä»½è¯ç­‰ææ–™
æ¥æº: æŠ¥åˆ°æ‰‹å†Œ.pdf, é¡µç : 5
è·ç¦»: 0.1234

ç»“æœ 2:
æ–‡æ¡£: å­¦æ ¡æœ‰å¤šä¸ªé‡ç‚¹å®éªŒå®¤ï¼ŒåŒ…æ‹¬äººå·¥æ™ºèƒ½å®éªŒå®¤ç­‰
æ¥æº: é™¢æ ¡ç®€ä»‹.docx, é¡µç : 1
è·ç¦»: 0.5678

ç»“æœ 3:
æ–‡æ¡£: å­¦ç”Ÿè¿çºªåˆ†ä¸ºè­¦å‘Šã€ä¸¥é‡è­¦å‘Šã€è®°è¿‡ç­‰
æ¥æº: è¿çºªè§„å®š.pdf, é¡µç : 3
è·ç¦»: 0.8901
```

### æŒä¹…åŒ–å­˜å‚¨

```python
# åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(path="./db/chroma_db")

# åç»­æ“ä½œä¸å†…å­˜å®¢æˆ·ç«¯ç›¸åŒ
collection = client.get_collection(name="campus_knowledge")
```

---

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæ™ºèƒ½é—®ç­”

**é—®é¢˜**ï¼š"æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"

**æµç¨‹**ï¼š
1. å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡
2. åœ¨å‘é‡åº“ä¸­æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
3. è¿”å› top-K ä¸ªç›¸å…³æ–‡æ¡£
4. LLM åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”

### åœºæ™¯ 2ï¼šè¯­ä¹‰å»é‡

**éœ€æ±‚**ï¼šæ£€æµ‹æ–‡æ¡£åº“ä¸­é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„å†…å®¹

**æµç¨‹**ï¼š
1. å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–
2. è®¡ç®—æ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
3. ç›¸ä¼¼åº¦ > 0.95 çš„è§†ä¸ºé‡å¤
4. ä¿ç•™ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–

### åœºæ™¯ 3ï¼šæ¨èç³»ç»Ÿ

**éœ€æ±‚**ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ¨èç›¸å…³å†…å®¹

**æµç¨‹**ï¼š
1. å°†ç”¨æˆ·æŸ¥è¯¢å‘é‡åŒ–
2. æœç´¢ç›¸ä¼¼åº¦æœ€é«˜çš„å†…å®¹
3. æŒ‰ç›¸ä¼¼åº¦æ’åºæ¨è

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡æ“ä½œ

```python
# æ‰¹é‡æ’å…¥ï¼ˆæ¨èï¼‰
collection.add(
    ids=[f"doc_{i}" for i in range(1000)],
    embeddings=embeddings,  # 1000 ä¸ªå‘é‡
    documents=documents,
    metadatas=metadatas
)

# è€Œéå¾ªç¯æ’å…¥ï¼ˆä¸æ¨èï¼‰
for i in range(1000):
    collection.add(
        ids=[f"doc_{i}"],
        embeddings=[embeddings[i]],
        documents=[documents[i]],
        metadatas=[metadatas[i]]
    )
```

### 2. å‚æ•°è°ƒä¼˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `n_results` | è¿”å›ç»“æœæ•°é‡ | 3-10 |
| `embedding_dim` | å‘é‡ç»´åº¦ | 768-1024 |
| `metric` | ç›¸ä¼¼åº¦åº¦é‡ | cosine |
| `batch_size` | æ‰¹å¤„ç†å¤§å° | 100-1000 |

### 3. ç´¢å¼•ä¼˜åŒ–

```python
# åˆ›å»º HNSW ç´¢å¼•
collection = client.create_collection(
    name="campus_knowledge",
    metadata={
        "hnsw:space": "cosine",
        "hnsw:construction_ef": 100,  # æ„å»ºæ—¶æœç´¢èŒƒå›´
        "hnsw:M": 16  # æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
    }
)
```

---

## ğŸš€ æœ€ä½³å®è·µ

### 1. å‘é‡å½’ä¸€åŒ–

```python
from sklearn.preprocessing import normalize

# å½’ä¸€åŒ–å‘é‡ï¼ˆL2 èŒƒæ•°ï¼‰
normalized_vectors = normalize(vectors, norm='l2')

# ä½¿ç”¨å½’ä¸€åŒ–å‘é‡æ—¶ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯
similarity = np.dot(vec1, vec2)  # ç­‰ä»·äº cosine_similarity
```

### 2. å…ƒæ•°æ®è¿‡æ»¤

```python
# ç»“åˆç›¸ä¼¼åº¦å’Œå…ƒæ•°æ®è¿‡æ»¤
results = collection.query(
    query_texts=["æŠ¥åˆ°"],
    n_results=10,
    where={"source": "æŠ¥åˆ°æ‰‹å†Œ.pdf"},  # åªæŸ¥è¯¢ç‰¹å®šæ¥æº
    where_document={"$contains": "å½•å–é€šçŸ¥ä¹¦"}  # æ–‡æ¡£åŒ…å«ç‰¹å®šå†…å®¹
)
```

### 3. æ··åˆæ£€ç´¢

```python
# å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢
results = collection.query(
    query_texts=["æŠ¥åˆ°ææ–™"],
    n_results=5
)

# åå¤„ç†ï¼šç»“åˆå…³é”®è¯åŒ¹é…è¿‡æ»¤
keyword_matches = [
    doc for doc in results['documents'][0]
    if "å½•å–é€šçŸ¥ä¹¦" in doc
]
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- ChromaDB æ–‡æ¡£ï¼šhttps://docs.trychroma.com/
- Sentence-Transformersï¼šhttps://www.sbert.net/

### æ¨èé˜…è¯»
- ã€Šå‘é‡æ•°æ®åº“å®æˆ˜ã€‹
- ã€ŠEmbedding æŠ€æœ¯è¯¦è§£ã€‹
- ã€Šè¯­ä¹‰æœç´¢æœ€ä½³å®è·µã€‹

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**ï¼š2026-01-30
**æ–‡æ¡£ç»´æŠ¤è€…**ï¼šCampusFlow é¡¹ç›®ç»„
