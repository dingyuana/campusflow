# LangChain 1.0 åŸºç¡€ç»„ä»¶è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œæä¾›äº†æ„å»º AI åº”ç”¨çš„æ ¸å¿ƒç»„ä»¶ã€‚LangChain 1.0 å¯¹æ ¸å¿ƒç»„ä»¶è¿›è¡Œäº†é‡æ„å’Œä¼˜åŒ–ï¼Œæä¾›äº†æ›´ç»Ÿä¸€çš„æ¥å£å’Œæ›´å¥½çš„æ€§èƒ½ã€‚

---

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangChain 1.0                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Loader  â”‚  â”‚ Splitter â”‚  â”‚Embeddingsâ”‚           â”‚
â”‚  â”‚ (æ•°æ®åŠ è½½)â”‚â†’â”‚ (æ–‡æ¡£åˆ‡åˆ†)â”‚â†’â”‚  (å‘é‡åŒ–) â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â†“              â†“              â†“                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Vector Store (å‘é‡å­˜å‚¨)           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚Retriever â”‚  â”‚  Chain   â”‚  â”‚  Agent   â”‚        â”‚
â”‚  â”‚ (æ£€ç´¢å™¨) â”‚â†’â”‚  (é“¾)    â”‚â†’â”‚ (æ™ºèƒ½ä½“)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Loaderï¼ˆæ–‡æ¡£åŠ è½½å™¨ï¼‰

### 1.1 åŸºç¡€æ¦‚å¿µ

**Loader** æ˜¯ç”¨äºä»å„ç§æ•°æ®æºåŠ è½½æ–‡æ¡£çš„ç»„ä»¶ï¼Œå°†éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸º LangChain çš„æ ‡å‡† Document æ ¼å¼ã€‚

**Document ç»“æ„**ï¼š
```python
Document(
    page_content="æ–‡æ¡£å†…å®¹",
    metadata={
        "source": "æ–‡ä»¶è·¯å¾„",
        "page": 1,
        "author": "ä½œè€…"
    }
)
```

### 1.2 å¸¸ç”¨ Loader

#### PDF Loader

```python
from langchain_community.document_loaders import PyPDFLoader

# åŠ è½½ PDF æ–‡æ¡£
loader = PyPDFLoader("docs/æ•™å­¦æ–‡ä»¶/ragfiles/2025å¹´æœ¬ç§‘æ–°ç”ŸæŠ¥åˆ°æ‰‹å†Œ.pdf")
documents = loader.load()

print(f"åŠ è½½äº† {len(documents)} é¡µ")
print(f"ç¬¬ä¸€é¡µå†…å®¹: {documents[0].page_content[:100]}...")
print(f"å…ƒæ•°æ®: {documents[0].metadata}")
```

**è¾“å‡º**ï¼š
```
åŠ è½½äº† 29 é¡µ
ç¬¬ä¸€é¡µå†…å®¹: 2025å¹´æœ¬ç§‘æ–°ç”ŸæŠ¥åˆ°æ‰‹å†Œ...

å…ƒæ•°æ®: {
    'source': 'docs/æ•™å­¦æ–‡ä»¶/ragfiles/2025å¹´æœ¬ç§‘æ–°ç”ŸæŠ¥åˆ°æ‰‹å†Œ.pdf',
    'page': 1
}
```

#### Text Loader

```python
from langchain_community.document_loaders import TextLoader

# åŠ è½½æ–‡æœ¬æ–‡ä»¶
loader = TextLoader("data/README.txt", encoding='utf-8')
documents = loader.load()
```

#### Word Loader

```python
from langchain_community.document_loaders import Docx2txtLoader

# åŠ è½½ Word æ–‡æ¡£
loader = Docx2txtLoader("docs/æ•™å­¦æ–‡ä»¶/ragfiles/é™¢æ ¡ç®€ä»‹.docx")
documents = loader.load()
```

#### Web Loader

```python
from langchain_community.document_loaders import WebBaseLoader

# åŠ è½½ç½‘é¡µ
loader = WebBaseLoader("https://example.com")
documents = loader.load()
```

#### Directory Loader

```python
from langchain_community.document_loaders import DirectoryLoader

# åŠ è½½æ•´ä¸ªç›®å½•
loader = DirectoryLoader(
    "docs/æ•™å­¦æ–‡ä»¶/ragfiles/",
    glob="**/*.pdf",  # åªåŠ è½½ PDF æ–‡ä»¶
    show_progress=True
)
documents = loader.load()
```

#### JSON Loader

```python
from langchain_community.document_loaders import JSONLoader

# åŠ è½½ JSON æ–‡ä»¶
loader = JSONLoader(
    file_path="data/data.json",
    jq=".documents[]",  # jq æŸ¥è¯¢è¯­å¥
    text_content=False
)
documents = loader.load()
```

### 1.3 è‡ªå®šä¹‰ Loader

```python
from langchain_core.documents import Document
from typing import List
from pathlib import Path

class CustomLoader:
    """
    è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨
    """

    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        # è‡ªå®šä¹‰åŠ è½½é€»è¾‘
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åˆ›å»º Document å¯¹è±¡
        return [
            Document(
                page_content=content,
                metadata={"source": self.file_path}
            )
        ]

# ä½¿ç”¨è‡ªå®šä¹‰ Loader
loader = CustomLoader("data/custom.txt")
documents = loader.load()
```

---

## 2. Splitterï¼ˆæ–‡æ¡£åˆ‡åˆ†å™¨ï¼‰

### 2.1 åŸºç¡€æ¦‚å¿µ

**Splitter** ç”¨äºå°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå°çš„ã€è¯­ä¹‰å®Œæ•´çš„æ–‡æœ¬å—ï¼ˆchunksï¼‰ï¼Œä»¥ä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢ã€‚

**åˆ‡åˆ†åŸåˆ™**ï¼š
1. **è¯­ä¹‰å®Œæ•´æ€§**ï¼šåœ¨è¯­ä¹‰è¾¹ç•Œï¼ˆæ®µè½ã€å¥å­ï¼‰åˆ‡åˆ†
2. **åˆç†çš„é‡å **ï¼šç›¸é‚»æ–‡æœ¬å—æœ‰ä¸€å®šé‡å ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
3. **åˆé€‚çš„å¤§å°**ï¼šé€šå¸¸ 400-800 å­—ç¬¦

### 2.2 RecursiveCharacterTextSplitter

è¿™æ˜¯ LangChain æ¨èçš„åˆ‡åˆ†å™¨ï¼Œèƒ½å¤Ÿé€’å½’å°è¯•å¤šç§åˆ†éš”ç¬¦ã€‚

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åˆ›å»ºåˆ‡åˆ†å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # æ–‡æœ¬å—å¤§å°
    chunk_overlap=50,        # æ–‡æœ¬å—é‡å 
    length_function=len,      # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=[             # åˆ†éš”ç¬¦åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        "\n\n",  # æ®µè½
        "\n",     # è¡Œ
        "ã€‚",    # ä¸­æ–‡å¥å·
        "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
        "ï¼Ÿ",    # ä¸­æ–‡é—®å·
        ".",     # è‹±æ–‡å¥å·
        "!",     # è‹±æ–‡æ„Ÿå¹å·
        "?",     # è‹±æ–‡é—®å·
        " ",     # ç©ºæ ¼
        ""       # å…œåº•
    ]
)

# åˆ‡åˆ†æ–‡æ¡£
splits = text_splitter.split_documents(documents)

print(f"åˆ‡åˆ†åæ–‡æœ¬å—æ•°é‡: {len(splits)}")
```

**åˆ‡åˆ†æ•ˆæœ**ï¼š
```python
# åŸå§‹æ–‡æ¡£
"æ–°ç”ŸæŠ¥åˆ°æ—¶é—´ï¼šæ¯å¹´ 9 æœˆ 1 æ—¥è‡³ 9 æœˆ 5 æ—¥ã€‚æŠ¥åˆ°åœ°ç‚¹ï¼šå­¦æ ¡ä¸»æ¥¼å¤§å…ã€‚æ‰€éœ€ææ–™ï¼šå½•å–é€šçŸ¥ä¹¦ã€èº«ä»½è¯åŸä»¶åŠå¤å°ä»¶ã€‚"

# æ–‡æœ¬å— 1
"æ–°ç”ŸæŠ¥åˆ°æ—¶é—´ï¼šæ¯å¹´ 9 æœˆ 1 æ—¥è‡³ 9 æœˆ 5 æ—¥ã€‚æŠ¥åˆ°åœ°ç‚¹ï¼šå­¦æ ¡ä¸»æ¥¼å¤§å…ã€‚"

# æ–‡æœ¬å— 2ï¼ˆæœ‰ 50 å­—ç¬¦é‡å ï¼‰
"æŠ¥åˆ°åœ°ç‚¹ï¼šå­¦æ ¡ä¸»æ¥¼å¤§å…ã€‚æ‰€éœ€ææ–™ï¼šå½•å–é€šçŸ¥ä¹¦ã€èº«ä»½è¯åŸä»¶åŠå¤å°ä»¶ã€‚"
```

### 2.3 CharacterTextSplitter

åŸºäºå­—ç¬¦çš„ç®€å•åˆ‡åˆ†å™¨ã€‚

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n",      # åˆ†éš”ç¬¦
    chunk_size=500,
    chunk_overlap=50,
    length_function=len
)

splits = text_splitter.split_documents(documents)
```

### 2.4 å…¶ä»–åˆ‡åˆ†å™¨

#### HTML æ ‡é¢˜åˆ‡åˆ†å™¨

```python
from langchain_text_splitters import HTMLHeaderTextSplitter

html_splitter = HTMLHeaderTextSplitter(
    headers_to_split_on=[
        ("h1", "Header 1"),
        ("h2", "Header 2"),
        ("h3", "Header 3")
    ]
)

splits = html_splitter.split_text(html_content)
```

#### Markdown åˆ‡åˆ†å™¨

```python
from langchain_text_splitters import MarkdownHeaderTextSplitter

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3")
    ]
)

splits = markdown_splitter.split_text(markdown_content)
```

### 2.5 åˆ‡åˆ†å™¨å‚æ•°è°ƒä¼˜

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `chunk_size` | 400-800 | æ–‡æœ¬å—å¤§å°ï¼Œæ ¹æ®æ¨¡å‹è°ƒæ•´ |
| `chunk_overlap` | 50-100 | é‡å å¤§å°ï¼Œé€šå¸¸ä¸º chunk_size çš„ 10-15% |
| `separators` | æŒ‰ä¼˜å…ˆçº§æ’åº | ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†éš”ç¬¦ |

---

## 3. Embeddingsï¼ˆåµŒå…¥æ¨¡å‹ï¼‰

### 3.1 åŸºç¡€æ¦‚å¿µ

**Embeddings** å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œä½¿ç›¸ä¼¼æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ã€‚

### 3.2 ä½¿ç”¨ HuggingFace Embeddings

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

# åˆå§‹åŒ– Embeddings æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",  # æ¨¡å‹åç§°
    model_kwargs={'device': 'cpu'},  # ä½¿ç”¨ CPU
    encode_kwargs={'normalize_embeddings': True}  # å½’ä¸€åŒ–
)

# ç”Ÿæˆæ–‡æ¡£å‘é‡
text = "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"
vector = embeddings.embed_query(text)

print(f"å‘é‡ç»´åº¦: {len(vector)}")
print(f"å‘é‡å‰5ä¸ªå€¼: {vector[:5]}")
```

**è¾“å‡º**ï¼š
```
å‘é‡ç»´åº¦: 1024
å‘é‡å‰5ä¸ªå€¼: [0.0234, -0.1567, 0.8721, 0.4532, -0.2312]
```

### 3.3 æ‰¹é‡ç”Ÿæˆå‘é‡

```python
# æ‰¹é‡ç”Ÿæˆæ–‡æ¡£å‘é‡
texts = [
    "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡å½•å–é€šçŸ¥ä¹¦",
    "å­¦æ ¡æœ‰å¤šä¸ªé‡ç‚¹å®éªŒå®¤",
    "å­¦ç”Ÿè¿çºªåˆ†ä¸ºè­¦å‘Šã€ä¸¥é‡è­¦å‘Šç­‰"
]

vectors = embeddings.embed_documents(texts)

print(f"ç”Ÿæˆäº† {len(vectors)} ä¸ªå‘é‡")
```

### 3.4 ä½¿ç”¨ OpenAI Embeddings

```python
from langchain_openai import OpenAIEmbeddings

# åˆå§‹åŒ– OpenAI Embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key="your-api-key"
)

# ç”Ÿæˆå‘é‡
vector = embeddings.embed_query("æŸ¥è¯¢æ–‡æœ¬")
```

---

## 4. Vector Storeï¼ˆå‘é‡å­˜å‚¨ï¼‰

### 4.1 åŸºç¡€æ¦‚å¿µ

**Vector Store** æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡çš„æ•°æ®åº“ï¼Œæ”¯æŒé«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢ã€‚

### 4.2 ChromaDB

```python
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# åˆå§‹åŒ– Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    encode_kwargs={'normalize_embeddings': True}
)

# åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
vector_store = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    collection_name="campus_knowledge"
)

# æŒä¹…åŒ–åˆ°ç£ç›˜
vector_store = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./db/chroma_db",
    collection_name="campus_knowledge"
)

# åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
vector_store = Chroma(
    persist_directory="./db/chroma_db",
    embedding_function=embeddings,
    collection_name="campus_knowledge"
)
```

### 4.3 ç›¸ä¼¼åº¦æœç´¢

```python
# ç›¸ä¼¼åº¦æœç´¢
query = "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"
results = vector_store.similarity_search(query, k=3)

for i, doc in enumerate(results, 1):
    print(f"ç»“æœ {i}:")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print(f"æ¥æº: {doc.metadata}")
    print()
```

### 4.4 MMR æœç´¢ï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰

```python
# MMR æœç´¢ï¼ˆå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼‰
results = vector_store.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10  # ä» 10 ä¸ªå€™é€‰ä¸­é€‰æ‹© 3 ä¸ª
)
```

### 4.5 å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢

```python
# å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢
results = vector_store.similarity_search_with_score(query, k=3)

for i, (doc, score) in enumerate(results, 1):
    print(f"ç»“æœ {i}:")
    print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print()
```

### 4.6 å…¶ä»– Vector Store

#### FAISS

```python
from langchain_community.vectorstores import FAISS

# åˆ›å»º FAISS å‘é‡æ•°æ®åº“
vector_store = FAISS.from_documents(
    documents=splits,
    embedding=embeddings
)

# ä¿å­˜åˆ°ç£ç›˜
vector_store.save_local("db/faiss_index")

# åŠ è½½
vector_store = FAISS.load_local(
    "db/faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True
)
```

#### Qdrant

```python
from langchain_community.vectorstores import Qdrant

# åˆ›å»º Qdrant å‘é‡æ•°æ®åº“
vector_store = Qdrant.from_documents(
    documents=splits,
    embedding=embeddings,
    url="http://localhost:6333",
    collection_name="campus_knowledge"
)
```

---

## 5. Retrieverï¼ˆæ£€ç´¢å™¨ï¼‰

### 5.1 åŸºç¡€æ¦‚å¿µ

**Retriever** æ˜¯ç”¨äºæ£€ç´¢ç›¸å…³æ–‡æ¡£çš„ç»„ä»¶ï¼Œå¯ä»¥åŸºäºå‘é‡ç›¸ä¼¼åº¦ã€å…³é”®è¯åŒ¹é…ç­‰æ–¹å¼è¿›è¡Œæ£€ç´¢ã€‚

### 5.2 Vector Store Retriever

```python
# ä» Vector Store åˆ›å»º Retriever
retriever = vector_store.as_retriever(
    search_type="similarity",      # æ£€ç´¢ç±»å‹
    search_kwargs={"k": 3}         # è¿”å›ç»“æœæ•°é‡
)

# æ‰§è¡Œæ£€ç´¢
results = retriever.invoke("æŠ¥åˆ°éœ€è¦ä»€ä¹ˆææ–™ï¼Ÿ")

for doc in results:
    print(doc.page_content)
```

### 5.3 MMRetriever

```python
# ä½¿ç”¨ MMR æ£€ç´¢
retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 3,
        "fetch_k": 10  # ä» 10 ä¸ªå€™é€‰ä¸­é€‰æ‹©
    }
)
```

### 5.4 æœ€å¤§è¾¹é™…ç›¸å…³æ€§åˆ†æ•°æ£€ç´¢

```python
# å¸¦åˆ†æ•°çš„ MMR æ£€ç´¢
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼
        "k": 3
    }
)
```

### 5.5 è‡ªå®šä¹‰ Retriever

```python
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from typing import List

class CustomRetriever(BaseRetriever):
    """è‡ªå®šä¹‰æ£€ç´¢å™¨"""

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """å®ç°æ£€ç´¢é€»è¾‘"""
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        results = vector_store.similarity_search(query, k=3)
        return results

# ä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å™¨
retriever = CustomRetriever()
results = retriever.invoke("æŸ¥è¯¢æ–‡æœ¬")
```

---

## 6. Chainï¼ˆé“¾ï¼‰

### 6.1 åŸºç¡€æ¦‚å¿µ

**Chain** æ˜¯å°†å¤šä¸ªç»„ä»¶ä¸²è”èµ·æ¥ï¼Œæ„å»ºå¤æ‚å·¥ä½œæµçš„æœºåˆ¶ã€‚

### 6.2 LLM Chain

```python
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»º LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template(
    "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{question}"
)

# åˆ›å»º Chain
chain = LLMChain(llm=llm, prompt=prompt)

# æ‰§è¡Œ Chain
result = chain.invoke({"question": "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"})
print(result["text"])
```

### 6.3 RetrievalQA Chain

```python
from langchain.chains import RetrievalQA

# åˆ›å»º RetrievalQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # é“¾ç±»å‹
    retriever=retriever,
    return_source_documents=True
)

# æ‰§è¡ŒæŸ¥è¯¢
query = "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"
result = qa_chain.invoke({"query": query})

print(f"å›ç­”: {result['result']}")
print(f"æ¥æº: {result['source_documents']}")
```

### 6.4 é“¾ç±»å‹

| ç±»å‹ | è¯´æ˜ |
|------|------|
| `stuff` | ç®€å•æ‹¼æ¥æ‰€æœ‰æ–‡æ¡£å— |
| `map_reduce` | åˆ†åˆ«å¤„ç†æ¯ä¸ªæ–‡æ¡£å—ï¼Œç„¶ååˆå¹¶ç»“æœ |
| `refine` | è¿­ä»£ä¼˜åŒ–å›ç­” |
| `map_rerank` | å¯¹æ¯ä¸ªæ–‡æ¡£å—è¯„åˆ†ï¼Œé€‰æ‹©æœ€å¥½çš„ |

### 6.5 Sequential Chain

```python
from langchain.chains import SequentialChain

# Chain 1ï¼šæå–å…³é”®ä¿¡æ¯
chain1 = LLMChain(
    llm=llm,
    prompt=ChatPromptTemplate.from_template(
        "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼š\n{text}\n\nå…³é”®ä¿¡æ¯ï¼š"
    ),
    output_key="key_info"
)

# Chain 2ï¼šç”Ÿæˆå›ç­”
chain2 = LLMChain(
    llm=llm,
    prompt=ChatPromptTemplate.from_template(
        "æ ¹æ®ä»¥ä¸‹å…³é”®ä¿¡æ¯ç”Ÿæˆå›ç­”ï¼š\n{key_info}\n\nå›ç­”ï¼š"
    ),
    output_key="answer"
)

# ä¸²è” Chain
overall_chain = SequentialChain(
    chains=[chain1, chain2],
    input_variables=["text"],
    output_variables=["answer"]
)

# æ‰§è¡Œ
result = overall_chain.invoke({"text": "æ–‡æœ¬å†…å®¹"})
print(result["answer"])
```

---

## 7. Agentï¼ˆæ™ºèƒ½ä½“ï¼‰

### 7.1 åŸºç¡€æ¦‚å¿µ

**Agent** æ˜¯èƒ½å¤Ÿè‡ªä¸»å†³ç­–å’Œæ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·æ¥å®Œæˆå¤æ‚ä»»åŠ¡ã€‚

### 7.2 ReAct Agent

```python
from langchain.agents import AgentType, initialize_agent
from langchain.tools import Tool

# å®šä¹‰å·¥å…·
def search_database(query: str) -> str:
    """æœç´¢æ•°æ®åº“"""
    return f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š{query}"

def search_internet(query: str) -> str:
    """æœç´¢äº’è”ç½‘"""
    return f"äº’è”ç½‘æœç´¢ç»“æœï¼š{query}"

tools = [
    Tool(
        name="DatabaseSearch",
        func=search_database,
        description="æœç´¢æ ¡å›­æ•°æ®åº“ï¼Œå›ç­”å…³äºå­¦æ ¡æ”¿ç­–ã€è§„å®šçš„é—®é¢˜"
    ),
    Tool(
        name="InternetSearch",
        func=search_internet,
        description="æœç´¢äº’è”ç½‘ï¼Œè·å–å®æ—¶ä¿¡æ¯"
    )
]

# åˆå§‹åŒ– Agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# æ‰§è¡ŒæŸ¥è¯¢
result = agent.invoke("ä»Šå¤©æ ¡å›­æœ‰ä»€ä¹ˆæ´»åŠ¨ï¼Ÿ")
print(result["output"])
```

### 7.3 OpenAI Functions Agent

```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ ¡å›­æ™ºèƒ½åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# åˆ›å»º Agent
agent = create_openai_functions_agent(llm, tools, prompt)

# åˆ›å»º Agent æ‰§è¡Œå™¨
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
)

# æ‰§è¡ŒæŸ¥è¯¢
result = agent_executor.invoke({
    "input": "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ",
    "chat_history": []
})
```

---

## ğŸ“Š ç»„ä»¶å¯¹æ¯”

| ç»„ä»¶ | ä½œç”¨ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **Loader** | åŠ è½½æ•°æ® | æ–‡ä»¶è·¯å¾„/URL | List[Document] |
| **Splitter** | åˆ‡åˆ†æ–‡æ¡£ | List[Document] | List[Document] |
| **Embeddings** | å‘é‡åŒ– | æ–‡æœ¬ | List[float] |
| **Vector Store** | å­˜å‚¨å‘é‡ | Document+Embedding | VectorStore |
| **Retriever** | æ£€ç´¢æ–‡æ¡£ | æŸ¥è¯¢æ–‡æœ¬ | List[Document] |
| **Chain** | ä¸²è”ç»„ä»¶ | è¾“å…¥å­—å…¸ | è¾“å‡ºå­—å…¸ |
| **Agent** | è‡ªä¸»å†³ç­– | ç”¨æˆ·é—®é¢˜ | æœ€ç»ˆç­”æ¡ˆ |

---

## ğŸš€ å®Œæ•´ç¤ºä¾‹ï¼šRAG åº”ç”¨

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# 1. åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("docs/æ•™å­¦æ–‡ä»¶/ragfiles/2025å¹´æœ¬ç§‘æ–°ç”ŸæŠ¥åˆ°æ‰‹å†Œ.pdf")
documents = loader.load()

# 2. åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
)
splits = text_splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    encode_kwargs={'normalize_embeddings': True}
)
vector_store = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./db/chroma_db",
    collection_name="campus_knowledge"
)

# 4. åˆ›å»ºæ£€ç´¢å™¨
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# 5. åˆ›å»º LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 6. åˆ›å»º RAG Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 7. æ‰§è¡ŒæŸ¥è¯¢
query = "æ–°ç”ŸæŠ¥åˆ°éœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ"
result = qa_chain.invoke({"query": query})

print(f"é—®é¢˜: {query}")
print(f"å›ç­”: {result['result']}")
print(f"æ¥æº: {[doc.metadata for doc in result['source_documents']]}")
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- LangChain æ–‡æ¡£ï¼šhttps://python.langchain.com/
- LangChain 1.0 æ›´æ–°æ—¥å¿—ï¼šhttps://python.langchain.com/docs/versions/

### æ¨èé˜…è¯»
- ã€ŠLangChain å®æˆ˜ã€‹
- ã€Šå¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘æŒ‡å—ã€‹
- ã€ŠRAG ä¸ LangChain æœ€ä½³å®è·µã€‹

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**ï¼š2026-01-30
**æ–‡æ¡£ç»´æŠ¤è€…**ï¼šCampusFlow é¡¹ç›®ç»„
