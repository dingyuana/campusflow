"""
Day 2: æ–‡æ¡£åŠ è½½ä¸æ™ºèƒ½åˆ‡åˆ†
é’ˆå¯¹ã€Šæ ¡å›­æŠ¥åˆ°æ‰‹å†Œã€‹PDF çš„æ™ºèƒ½åˆ‡åˆ†ç­–ç•¥
"""

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import re


def load_and_split_handbook(pdf_path: str):
    """
    åŠ è½½æŠ¥åˆ°æ‰‹å†Œå¹¶è¿›è¡Œè¯­ä¹‰å‹å¥½çš„åˆ‡åˆ†
    
    ç­–ç•¥ï¼šæŒ‰æ®µè½åˆ‡åˆ†ï¼Œä¿ç•™ä¸Šä¸‹æ–‡æ ‡é¢˜
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆ‡åˆ†åçš„æ–‡æ¡£å—åˆ—è¡¨
    """
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    
    # é¢„å¤„ç†ï¼šåˆå¹¶é¡µçœ‰é¡µè„šï¼Œæå–æ ‡é¢˜å±‚çº§
    processed_docs = []
    for doc in documents:
        # æ¸…ç†é¡µç ç­‰å™ªå£°
        cleaned = re.sub(r'\n\s*\d+\s*\n', '\n', doc.page_content)
        # è¯†åˆ«æ ‡é¢˜ï¼ˆå¦‚"ä¸‰ã€ç¼´è´¹è¯´æ˜"ï¼‰
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', cleaned.strip()):
            doc.metadata["is_header"] = True
        processed_docs.append(doc)
    
    # é€’å½’å­—ç¬¦åˆ‡åˆ†ï¼šchunk_size=500ï¼Œä¿ç•™æ®µè½å®Œæ•´æ€§
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""],
        length_function=len,
        is_separator_regex=False
    )
    
    chunks = text_splitter.split_documents(processed_docs)
    
    # å¢å¼º metadataï¼šç»§æ‰¿ç« èŠ‚æ ‡é¢˜
    for chunk in chunks:
        # ç®€å•çš„ä¸Šä¸‹æ–‡å¢å¼ºï¼šå¦‚æœ chunk ä»¥"("å¼€å¤´ï¼Œå¯èƒ½æ¥ä¸Šä¸€æ®µ
        if chunk.page_content.startswith(("(", "ï¼ˆ", "[", "ã€")):
            chunk.metadata["context_hint"] = "continued"
    
    print(f"ğŸ“„ åŸå§‹æ–‡æ¡£é¡µæ•°ï¼š{len(documents)}")
    print(f"âœ‚ï¸ åˆ‡åˆ†å chunks æ•°ï¼š{len(chunks)}")
    print(f"ğŸ“Š å¹³å‡ chunk é•¿åº¦ï¼š{sum(len(c.page_content) for c in chunks)/len(chunks):.0f} å­—ç¬¦")
    
    return chunks
