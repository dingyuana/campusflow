"""
Day 2: å‘é‡æ•°æ®åº“ä¸æ··åˆæ£€ç´¢
ä½¿ç”¨ ChromaDB å’Œ BGE-m3 åµŒå…¥æ¨¡å‹
"""

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from typing import List, Tuple


# ä½¿ç”¨ BGE-m3 æ¨¡å‹ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': 'cpu'},  # æ—  GPU ç¯å¢ƒä½¿ç”¨ cpu
    encode_kwargs={'normalize_embeddings': True}  # å½’ä¸€åŒ–ä¾¿äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
)


def create_vector_db(chunks, persist_dir="./chroma_db"):
    """
    åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡æ•°æ®åº“
    
    Args:
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        persist_dir: æŒä¹…åŒ–ç›®å½•
        
    Returns:
        Chroma å‘é‡æ•°æ®åº“å®ä¾‹
    """
    
    # å¦‚æœå·²å­˜åœ¨åˆ™åŠ è½½ï¼Œå¦åˆ™åˆ›å»º
    if os.path.exists(persist_dir) and len(os.listdir(persist_dir)) > 0:
        print("ğŸ”„ åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“...")
        vectordb = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings
        )
    else:
        print("ğŸ”¨ åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        vectordb = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=persist_dir,
            collection_metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦è·ç¦»
        )
        vectordb.persist()
    
    return vectordb


def hybrid_search(vectordb, query: str, k: int = 5):
    """
    æ··åˆæ£€ç´¢ï¼šè¯­ä¹‰æ£€ç´¢ + å…³é”®è¯è¿‡æ»¤
    
    Args:
        vectordb: Chroma å‘é‡æ•°æ®åº“
        query: æŸ¥è¯¢æ–‡æœ¬
        k: è¿”å›ç»“æœæ•°
        
    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨ [(doc, score), ...]
    """
    # 1. è¯­ä¹‰æ£€ç´¢
    semantic_results = vectordb.similarity_search_with_score(query, k=k*2)
    
    # 2. å…³é”®è¯åŒ¹é…å¼ºåŒ–ï¼ˆç®€å•å®ç°ï¼šåŒ…å«å…³é”®è¯çš„ boostï¼‰
    keywords = extract_keywords(query)  # ç®€å•åˆ†è¯æå–å…³é”®è¯
    boosted_results = []
    
    for doc, score in semantic_results:
        # åŸå§‹åˆ†æ•°æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼Œè½¬ä¸ºç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        similarity = 1 - score
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        content_lower = doc.page_content.lower()
        keyword_boost = sum(0.1 for kw in keywords if kw in content_lower)
        final_score = similarity + keyword_boost
        
        boosted_results.append((doc, final_score))
    
    # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºå¹¶è¿”å›å‰ k ä¸ª
    boosted_results.sort(key=lambda x: x[1], reverse=True)
    return boosted_results[:k]


def extract_keywords(text: str) -> List[str]:
    """
    ç®€å•å…³é”®è¯æå–ï¼ˆå®é™…å¯ç”¨ jiebaï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    # é’ˆå¯¹æ ¡å›­åœºæ™¯çš„å…³é”®è¯åº“
    key_terms = ["å­¦è´¹", "å®¿èˆ", "ä¸€å¡é€š", "å†›è®­", "å›¾ä¹¦é¦†", "æŠ¥åˆ°", "æ¡£æ¡ˆ", "æˆ·å£"]
    return [term for term in key_terms if term in text]
