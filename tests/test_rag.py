"""
Day 2: RAG æ£€ç´¢æ•ˆæœè¯„æµ‹
è¯„ä¼°æ£€ç´¢å‡†ç¡®ç‡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from db.vector_store import hybrid_search, create_vector_db
from db.rag_loader import load_and_split_handbook


def evaluate_retrieval(vectordb, test_cases):
    """
    è¯„æµ‹æ£€ç´¢å‡†ç¡®ç‡
    
    Args:
        vectordb: å‘é‡æ•°æ®åº“å®ä¾‹
        test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
        
    Returns:
        å¹³å‡å¬å›ç‡
    """
    scores = []
    
    for case in test_cases:
        results = hybrid_search(vectordb, case["query"], k=3)
        content = " ".join([doc.page_content for doc, _ in results])
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸå…³é”®è¯
        hit_count = sum(1 for kw in case["expected_keywords"] if kw in content)
        score = hit_count / len(case["expected_keywords"])
        scores.append(score)
        
        print(f"{'âœ…' if score > 0.5 else 'âŒ'} {case['description']}: {score:.0%}")
    
    avg_score = sum(scores) / len(scores) if scores else 0
    print(f"\nğŸ“Š å¹³å‡å¬å›ç‡ï¼š{avg_score:.0%}")
    return avg_score


def run_rag_tests():
    """è¿è¡Œ RAG ç³»ç»Ÿæµ‹è¯•"""
    print("="*60)
    print("ğŸ§ª RAG ç³»ç»Ÿæ£€ç´¢æ•ˆæœæµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ PDF æ–‡ä»¶
    pdf_path = "data/æ–°ç”ŸæŠ¥åˆ°æ‰‹å†Œ.pdf"
    if not os.path.exists(pdf_path):
        print(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{pdf_path}")
        print("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•...")
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        from langchain.schema import Document
        mock_chunks = [
            Document(page_content="æ¡£æ¡ˆè½¬é€’éœ€è¦é€šè¿‡EMSå­¦ç”Ÿæ¡£æ¡ˆä¸“é€’é€šé“ï¼Œç”±åŸé«˜ä¸­æˆ–äººæ‰ä¸­å¿ƒå¯„å‡ºã€‚æ¥æ”¶åœ°å€ï¼šXXå¤§å­¦æ¡£æ¡ˆé¦†ã€‚", metadata={"source": "æŠ¥åˆ°æ‰‹å†Œ", "page": 15}),
            Document(page_content="å†›è®­ä¸ºæœŸä¸¤å‘¨ï¼Œä»9æœˆ5æ—¥å¼€å§‹åˆ°9æœˆ18æ—¥ç»“æŸã€‚æœŸé—´è¿›è¡Œé˜Ÿåˆ—è®­ç»ƒã€å†…åŠ¡æ•´ç†ã€å›½é˜²æ•™è‚²ç­‰ã€‚", metadata={"source": "æŠ¥åˆ°æ‰‹å†Œ", "page": 25}),
            Document(page_content="å­¦è´¹ç¼´çº³æ”¯æŒé“¶è¡Œè½¬è´¦ã€æ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ä¸‰ç§æ–¹å¼ã€‚æˆªæ­¢æ—¥æœŸä¸º9æœˆ15æ—¥ã€‚", metadata={"source": "æŠ¥åˆ°æ‰‹å†Œ", "page": 10}),
        ]
        vectordb = create_vector_db(mock_chunks, persist_dir="./chroma_db_test")
    else:
        # åŠ è½½çœŸå®æ•°æ®
        chunks = load_and_split_handbook(pdf_path)
        vectordb = create_vector_db(chunks)
    
    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "query": "æ¡£æ¡ˆæ€ä¹ˆè½¬è¿‡æ¥ï¼Ÿ",
            "expected_keywords": ["æ¡£æ¡ˆ", "è½¬é€’"],
            "description": "æ¡£æ¡ˆè½¬é€’ç›¸å…³é—®é¢˜"
        },
        {
            "query": "å¼€å­¦è¦å†›è®­å¤šä¹…ï¼Ÿ",
            "expected_keywords": ["å†›è®­", "ä¸¤å‘¨"],
            "description": "å†›è®­æ—¶é•¿é—®é¢˜"
        },
        {
            "query": "å­¦è´¹æ€ä¹ˆäº¤ï¼Ÿ",
            "expected_keywords": ["å­¦è´¹", "ç¼´çº³"],
            "description": "ç¼´è´¹æ–¹å¼é—®é¢˜"
        }
    ]
    
    # è¿è¡Œè¯„æµ‹
    avg_score = evaluate_retrieval(vectordb, test_cases)
    
    print("\n" + "="*60)
    if avg_score >= 0.8:
        print("âœ… æµ‹è¯•é€šè¿‡ï¼æ£€ç´¢æ•ˆæœè‰¯å¥½")
    elif avg_score >= 0.5:
        print("âš ï¸  æµ‹è¯•é€šè¿‡ï¼Œä½†æ£€ç´¢æ•ˆæœæœ‰å¾…æå‡")
    else:
        print("âŒ æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦ä¼˜åŒ–æ£€ç´¢ç­–ç•¥")
    print("="*60)
    
    return avg_score


if __name__ == "__main__":
    run_rag_tests()
